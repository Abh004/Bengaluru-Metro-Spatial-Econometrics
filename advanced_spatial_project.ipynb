{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65f74886",
      "metadata": {
        "id": "65f74886"
      },
      "source": [
        "# Advanced Spatial Econometrics and Geospatial Modeling Project\n",
        "This notebook contains all components of the project combined into a single workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7304fb",
      "metadata": {
        "id": "1c7304fb"
      },
      "source": [
        "## data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7f941f9f",
      "metadata": {
        "id": "7f941f9f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def convert_sqft(x):\n",
        "    try:\n",
        "        tokens = x.split('-')\n",
        "        if len(tokens) == 2:\n",
        "            return (float(tokens[0]) + float(tokens[1])) / 2\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def load_and_clean_property_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df['total_sqft'] = df['total_sqft'].apply(convert_sqft)\n",
        "    df['bhk'] = df['size'].apply(lambda x: int(x.split(' ')[0]) if isinstance(x, str) else None)\n",
        "    df['price_per_sqft'] = df['price']*100000/df['total_sqft']\n",
        "    df = df.dropna(subset=['total_sqft','bhk','bath','price','location'])\n",
        "    df = df[df['total_sqft']<10000]\n",
        "    df['location']=df['location'].apply(lambda x: x.strip())\n",
        "    return df\n",
        "\n",
        "def load_metro_stations(green_path, purple_path):\n",
        "    green = pd.read_csv(green_path)\n",
        "    purple = pd.read_csv(purple_path)\n",
        "    stations = pd.concat([green, purple], ignore_index=True)\n",
        "    return list(zip(stations['Y'], stations['X']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b807433b",
      "metadata": {
        "id": "b807433b"
      },
      "source": [
        "## geospatial.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0ec886e9",
      "metadata": {
        "id": "0ec886e9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from sklearn.cluster import KMeans\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
        "\n",
        "# Cache file to store coordinates\n",
        "CACHE_FILE = 'loc_coords_cache.csv'\n",
        "\n",
        "# --- MANUAL DATABASE OF TOP 40 LOCATIONS ---\n",
        "# This ensures the project works even if the API blocks us.\n",
        "KNOWN_LOCATIONS = {\n",
        "    'Whitefield': (12.9698, 77.7500), 'Sarjapur  Road': (12.9166, 77.6736),\n",
        "    'Electronic City': (12.8452, 77.6602), 'Kanakpura Road': (12.8654, 77.5336),\n",
        "    'Thanisandra': (13.0547, 77.6339), 'Yelahanka': (13.1007, 77.5963),\n",
        "    'Uttarahalli': (12.9055, 77.5512), 'Hebbal': (13.0359, 77.5970),\n",
        "    'Marathahalli': (12.9591, 77.6974), 'Raja Rajeshwari Nagar': (12.9274, 77.5155),\n",
        "    'Bannerghatta Road': (12.8766, 77.5990), 'Hennur Road': (13.0603, 77.6423),\n",
        "    '7th Phase JP Nagar': (12.9024, 77.5802), 'Haralur Road': (12.9081, 77.6476),\n",
        "    'Electronic City Phase II': (12.8469, 77.6773), 'Rajaji Nagar': (12.9982, 77.5530),\n",
        "    'Chandapura': (12.7968, 77.6944), 'Bellandur': (12.9304, 77.6684),\n",
        "    'Hoodi': (12.9920, 77.7130), 'KR Puram': (13.0075, 77.6959),\n",
        "    'Yeshwanthpur': (13.0238, 77.5529), 'Kothanur': (13.0559, 77.6322),\n",
        "    'Koramangala': (12.9352, 77.6245), 'Indira Nagar': (12.9784, 77.6408),\n",
        "    'Malleshwaram': (13.0031, 77.5643), 'Jayanagar': (12.9308, 77.5802),\n",
        "    'Banashankari': (12.9155, 77.5736), 'JP Nagar': (12.9076, 77.5736),\n",
        "    'Peenya': (13.0285, 77.5460), 'Ulsoor': (12.9733, 77.6204),\n",
        "    'BTM Layout': (12.9166, 77.6101), 'Ramamurthy Nagar': (13.0165, 77.6777),\n",
        "    'Basaveshwara Nagar': (12.9847, 77.5491), 'Chamarajpet': (12.9569, 77.5635),\n",
        "    'Varthur': (12.9389, 77.7412), 'Madiwala': (12.9226, 77.6174),\n",
        "    'Basavanagudi': (12.9421, 77.5658), 'RT Nagar': (13.0108, 77.5763),\n",
        "    'Domlur': (12.9606, 77.6416), 'Frazer Town': (12.9990, 77.6124),\n",
        "    'HSR Layout': (12.9121, 77.6446), 'Banaswadi': (13.0104, 77.6482)\n",
        "}\n",
        "\n",
        "def map_locality_coords(df):\n",
        "    \"\"\"\n",
        "    Maps locations using a manual database first.\n",
        "    ATTEMPTS to fetch missing ones, but stops gracefully if errors occur.\n",
        "    \"\"\"\n",
        "    unique_locs = df['location'].unique()\n",
        "\n",
        "    # 1. Start with our Manual Database\n",
        "    loc_coords_dict = KNOWN_LOCATIONS.copy()\n",
        "\n",
        "    # 2. Load Cache if exists\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        try:\n",
        "            coords_df = pd.read_csv(CACHE_FILE, index_col=0)\n",
        "            cache_dict = {loc: (lat, lon) for loc, lat, lon in coords_df.itertuples()}\n",
        "            loc_coords_dict.update(cache_dict)\n",
        "        except:\n",
        "            pass # Ignore cache errors\n",
        "\n",
        "    # 3. Identify missing locations\n",
        "    missing_locs = [loc for loc in unique_locs if loc not in loc_coords_dict]\n",
        "\n",
        "    # 4. Attempt to fetch others (WITH FAIL-SAFE)\n",
        "    if missing_locs:\n",
        "        print(f\"[Geo] Processing {len(missing_locs)} remaining locations...\")\n",
        "        print(\"[Geo] NOTE: If API fails, we will proceed with known locations only.\")\n",
        "\n",
        "        # Increased timeout significantly\n",
        "        geolocator = Nominatim(user_agent=\"bengaluru_student_project_final\", timeout=10)\n",
        "        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=2.0)\n",
        "\n",
        "        error_count = 0\n",
        "        MAX_ERRORS = 3 # Stop after 3 consecutive errors to prevent hanging\n",
        "\n",
        "        for i, loc in enumerate(missing_locs):\n",
        "            if error_count >= MAX_ERRORS:\n",
        "                print(\"!!! Too many connection errors. Stopping live fetch to ensure completion.\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                query = f\"{loc}, Bengaluru\"\n",
        "                location = geocode(query)\n",
        "\n",
        "                if location:\n",
        "                    loc_coords_dict[loc] = (location.latitude, location.longitude)\n",
        "                    error_count = 0 # Reset error count on success\n",
        "                else:\n",
        "                    loc_coords_dict[loc] = (None, None)\n",
        "\n",
        "            except (GeocoderTimedOut, GeocoderUnavailable) as e:\n",
        "                print(f\"   -> Network Error on '{loc}'. Skipping.\")\n",
        "                error_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"   -> Error: {e}\")\n",
        "                error_count += 1\n",
        "\n",
        "    # 5. Save what we have\n",
        "    clean_dict = {k:v for k,v in loc_coords_dict.items() if v[0] is not None}\n",
        "    pd.DataFrame.from_dict(clean_dict, orient='index', columns=['lat', 'lon']).to_csv(CACHE_FILE)\n",
        "\n",
        "    # 6. Map and Drop Missing\n",
        "    df['lat'] = df['location'].map(lambda x: loc_coords_dict.get(x, (None, None))[0])\n",
        "    df['lon'] = df['location'].map(lambda x: loc_coords_dict.get(x, (None, None))[1])\n",
        "\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['lat', 'lon'])\n",
        "    print(f\"[Geo] Mapped {len(df)} properties successfully (Dropped {original_len - len(df)} unknown).\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_metro_distance(df, station_coords):\n",
        "    print(\"[Geo] Calculating distances to metro stations...\")\n",
        "    def get_min_dist(row):\n",
        "        prop_lat, prop_lon = row['lat'], row['lon']\n",
        "        min_d = float('inf')\n",
        "        R = 6371\n",
        "        for s_lat, s_lon in station_coords:\n",
        "            dlat = radians(s_lat - prop_lat)\n",
        "            dlon = radians(s_lon - prop_lon)\n",
        "            a = sin(dlat/2)**2 + cos(radians(prop_lat)) * cos(radians(s_lat)) * sin(dlon/2)**2\n",
        "            c = 2 * asin(sqrt(a))\n",
        "            d = R * c\n",
        "            if d < min_d: min_d = d\n",
        "        return min_d\n",
        "\n",
        "    df['dist_to_metro'] = df.apply(get_min_dist, axis=1)\n",
        "    return df\n",
        "\n",
        "def generate_spatial_clusters(df, n_clusters=5):\n",
        "    print(f\"[Geo] Generating {n_clusters} spatial clusters...\")\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    df['geo_cluster'] = kmeans.fit_predict(df[['lat', 'lon']])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace95233",
      "metadata": {
        "id": "ace95233"
      },
      "source": [
        "## models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c80be8ca",
      "metadata": {
        "id": "c80be8ca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def run_ols_regression(df):\n",
        "    df['dist_x_sqft']=df['dist_to_metro']*df['total_sqft']\n",
        "    df['log_price']=np.log(df['price'])\n",
        "    X=df[['dist_to_metro','total_sqft','bhk','bath','dist_x_sqft']]\n",
        "    X=sm.add_constant(X)\n",
        "    y=df['log_price']\n",
        "    model=sm.OLS(y,X).fit()\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def train_gbm(df):\n",
        "    df['log_price']=np.log(df['price'])\n",
        "    X=df[['dist_to_metro','total_sqft','bhk','bath','lat','lon']]\n",
        "    y=df['log_price']\n",
        "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "    gbm=GradientBoostingRegressor(n_estimators=200,learning_rate=0.1,max_depth=4,random_state=42)\n",
        "    gbm.fit(X_train,y_train)\n",
        "    y_pred=gbm.predict(X_test)\n",
        "    print(\"GBM R2:\",r2_score(y_test,y_pred))\n",
        "    return gbm,X_train,X_test,y_train,y_test,y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958d38ba",
      "metadata": {
        "id": "958d38ba"
      },
      "source": [
        "## visualizer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1cf89b42",
      "metadata": {
        "id": "1cf89b42"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "def plot_bid_rent_curve(df):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.regplot(x='dist_to_metro',y='price_per_sqft',data=df,scatter_kws={'alpha':0.3},line_kws={'color':'red'},order=2)\n",
        "    plt.savefig('bid_rent_curve.png'); plt.close()\n",
        "\n",
        "def plot_pdp(model,X_train):\n",
        "    fig,ax=plt.subplots(figsize=(10,6))\n",
        "    PartialDependenceDisplay.from_estimator(\n",
        "        model,\n",
        "        X_train,\n",
        "        ['dist_to_metro'],\n",
        "        ax=ax,\n",
        "        kind='average',\n",
        "        subsample=1000,\n",
        "        grid_resolution=100)\n",
        "    plt.savefig('pdp_distance.png'); plt.close()\n",
        "\n",
        "def plot_spatial_residuals(df,y_test,y_pred):\n",
        "    df_test=df.loc[y_test.index].copy()\n",
        "    df_test['pred']=np.exp(y_pred)\n",
        "    df_test['actual']=np.exp(y_test)\n",
        "    df_test['err']=df_test['actual']-df_test['pred']\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sc=plt.scatter(df_test['lon'],df_test['lat'],c=df_test['err'],cmap='coolwarm',s=80)\n",
        "    plt.colorbar(sc)\n",
        "    plt.savefig('spatial_residuals.png'); plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da9e5f2",
      "metadata": {
        "id": "4da9e5f2"
      },
      "source": [
        "## spatial_econometrics.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5dc7bd89",
      "metadata": {
        "id": "5dc7bd89"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def build_knn_weights(df, k=8):\n",
        "    from libpysal.weights import KNN\n",
        "    coords=df[['lat','lon']].to_numpy()\n",
        "    w=KNN.from_array(coords,k=k)\n",
        "    w.transform='r'\n",
        "    return w,coords\n",
        "\n",
        "def morans_i(df,w,variable='price_per_sqft'):\n",
        "    from esda.moran import Moran\n",
        "    m=Moran(df[variable].values,w)\n",
        "    print(\"Moran's I:\",m.I,\" p:\",m.p_sim)\n",
        "    return m\n",
        "\n",
        "def add_spatial_lag_feature(df,w,var='log_price',new_col='W_log_price'):\n",
        "    y=df[var].to_numpy()\n",
        "    wy=w.sparse.dot(y)\n",
        "    df[new_col]=wy\n",
        "    return df\n",
        "\n",
        "def run_sar_sem(df,feature_cols,dependent,w):\n",
        "    from spreg import ML_Lag,ML_Error\n",
        "    X=df[feature_cols].astype(float).values\n",
        "    y=df[dependent].astype(float).values.reshape(-1,1)\n",
        "    sar=ML_Lag(y,X,w=w,name_y=dependent,name_x=feature_cols)\n",
        "    sem=ML_Error(y,X,w=w,name_y=dependent,name_x=feature_cols)\n",
        "    print(sar.summary); print(sem.summary)\n",
        "    return sar,sem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e4ae70",
      "metadata": {
        "id": "95e4ae70"
      },
      "source": [
        "## main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ed928b58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed928b58",
        "outputId": "2295d041-daed-475f-889c-90da06aa31b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Geo] Processing 354 remaining locations...\n",
            "[Geo] NOTE: If API fails, we will proceed with known locations only.\n",
            "[Geo] Mapped 11932 properties successfully (Dropped 1241 unknown).\n",
            "[Geo] Calculating distances to metro stations...\n",
            "[Geo] Generating 5 spatial clusters...\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:              log_price   R-squared:                       0.662\n",
            "Model:                            OLS   Adj. R-squared:                  0.662\n",
            "Method:                 Least Squares   F-statistic:                     4571.\n",
            "Date:                Tue, 18 Nov 2025   Prob (F-statistic):               0.00\n",
            "Time:                        01:44:40   Log-Likelihood:                -5590.1\n",
            "No. Observations:               11656   AIC:                         1.119e+04\n",
            "Df Residuals:                   11650   BIC:                         1.124e+04\n",
            "Df Model:                           5                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const             3.2491      0.013    254.964      0.000       3.224       3.274\n",
            "dist_to_metro    -0.0232      0.002    -15.412      0.000      -0.026      -0.020\n",
            "total_sqft        0.0005   7.15e-06     67.822      0.000       0.000       0.000\n",
            "bhk               0.0741      0.007     10.933      0.000       0.061       0.087\n",
            "bath              0.1033      0.007     15.359      0.000       0.090       0.116\n",
            "dist_x_sqft    1.856e-06   8.88e-07      2.089      0.037    1.14e-07     3.6e-06\n",
            "==============================================================================\n",
            "Omnibus:                      589.594   Durbin-Watson:                   1.970\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2126.131\n",
            "Skew:                          -0.110   Prob(JB):                         0.00\n",
            "Kurtosis:                       5.081   Cond. No.                     4.02e+04\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 4.02e+04. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "GBM R2: 0.8219052269343098\n",
            "[Stats] Calculating Spatial Weights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/libpysal/weights/distance.py:153: UserWarning: The weights matrix is not fully connected: \n",
            " There are 186 disconnected components.\n",
            "  W.__init__(self, neighbors, id_order=ids, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moran's I: 0.2913779902303134  p: 0.001\n",
            "[Stats] Running Spatial Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spreg/ml_error.py:184: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
            "  res = minimize_scalar(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REGRESSION RESULTS\n",
            "------------------\n",
            "\n",
            "SUMMARY OF OUTPUT: MAXIMUM LIKELIHOOD SPATIAL LAG (METHOD = FULL)\n",
            "-----------------------------------------------------------------\n",
            "Data set            :     unknown\n",
            "Weights matrix      :     unknown\n",
            "Dependent Variable  :   log_price                Number of Observations:       11656\n",
            "Mean dependent var  :      4.3552                Number of Variables   :           6\n",
            "S.D. dependent var  :      0.6727                Degrees of Freedom    :       11650\n",
            "Pseudo R-squared    :      0.7258\n",
            "Spatial Pseudo R-squared:  0.6728\n",
            "Log likelihood      :  -4431.7536\n",
            "Sigma-square ML     :      0.1241                Akaike info criterion :    8875.507\n",
            "S.E of regression   :      0.3523                Schwarz criterion     :    8919.689\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "            Variable     Coefficient       Std.Error     z-Statistic     Probability\n",
            "------------------------------------------------------------------------------------\n",
            "            CONSTANT         1.31483         0.03336        39.41370         0.00000\n",
            "       dist_to_metro        -0.00681         0.00075        -9.05598         0.00000\n",
            "          total_sqft         0.00045         0.00001        87.82211         0.00000\n",
            "                 bhk         0.08994         0.00611        14.71058         0.00000\n",
            "                bath         0.08074         0.00607        13.29126         0.00000\n",
            "         W_log_price         0.44464         0.00743        59.84876         0.00000\n",
            "------------------------------------------------------------------------------------\n",
            "\n",
            "SPATIAL LAG MODEL IMPACTS\n",
            "Impacts computed using the 'simple' method.\n",
            "            Variable         Direct        Indirect          Total\n",
            "       dist_to_metro        -0.0068         -0.0055         -0.0123\n",
            "          total_sqft         0.0004          0.0004          0.0008\n",
            "                 bhk         0.0899          0.0720          0.1619\n",
            "                bath         0.0807          0.0646          0.1454\n",
            "================================ END OF REPORT =====================================\n",
            "REGRESSION RESULTS\n",
            "------------------\n",
            "\n",
            "SUMMARY OF OUTPUT: ML SPATIAL ERROR (METHOD = full)\n",
            "---------------------------------------------------\n",
            "Data set            :     unknown\n",
            "Weights matrix      :     unknown\n",
            "Dependent Variable  :   log_price                Number of Observations:       11656\n",
            "Mean dependent var  :      4.3552                Number of Variables   :           5\n",
            "S.D. dependent var  :      0.6727                Degrees of Freedom    :       11651\n",
            "Pseudo R-squared    :      0.6618\n",
            "Log likelihood      :  -4434.0688\n",
            "Sigma-square ML     :      0.1221                Akaike info criterion :    8878.138\n",
            "S.E of regression   :      0.3495                Schwarz criterion     :    8914.955\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "            Variable     Coefficient       Std.Error     z-Statistic     Probability\n",
            "------------------------------------------------------------------------------------\n",
            "            CONSTANT         3.30173         0.01601       206.27675         0.00000\n",
            "       dist_to_metro        -0.01750         0.00202        -8.65069         0.00000\n",
            "          total_sqft         0.00045         0.00001        88.83010         0.00000\n",
            "                 bhk         0.09098         0.00603        15.07766         0.00000\n",
            "                bath         0.07750         0.00598        12.95295         0.00000\n",
            "              lambda         0.65561         0.00868        75.52506         0.00000\n",
            "------------------------------------------------------------------------------------\n",
            "================================ END OF REPORT =====================================\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "# MAIN WORKFLOW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = load_and_clean_property_data('Bengaluru_House_Data.csv')\n",
        "stations = load_metro_stations('green.csv','purple.csv')\n",
        "\n",
        "df = map_locality_coords(df)\n",
        "df = calculate_metro_distance(df, stations)\n",
        "df = generate_spatial_clusters(df,5)\n",
        "\n",
        "# Remove unrealistic prices\n",
        "df = df[df['price_per_sqft'] < 20000]\n",
        "# Remove impossible metro distances\n",
        "df = df[df['dist_to_metro'] < 30]    # metro within city limits\n",
        "# Remove sqft errors\n",
        "df = df[df['total_sqft'] > 200]      # typical size for apartments\n",
        "\n",
        "\n",
        "# OLS\n",
        "ols_model = run_ols_regression(df)\n",
        "\n",
        "# GBM\n",
        "gbm,X_train,X_test,y_train,y_test,y_pred = train_gbm(df)\n",
        "\n",
        "# Visuals\n",
        "plot_bid_rent_curve(df)\n",
        "plot_pdp(gbm,X_train)\n",
        "plot_spatial_residuals(df,y_test,y_pred)\n",
        "\n",
        "# Spatial Econometrics\n",
        "print(\"[Stats] Calculating Spatial Weights...\")\n",
        "df['log_price'] = np.log(df['price'])\n",
        "\n",
        "# Build Weights\n",
        "w, coords = build_knn_weights(df, k=8)\n",
        "\n",
        "# Moran's I Check\n",
        "moran = morans_i(df, w, variable='log_price') # Suggest using log_price for consistency\n",
        "\n",
        "df = add_spatial_lag_feature(df, w, var='log_price', new_col='W_log_price')\n",
        "\n",
        "# Fix: Exclude 'W_log_price' from the regressors for SAR/SEM\n",
        "print(\"[Stats] Running Spatial Regression...\")\n",
        "sar, sem = run_sar_sem(df,\n",
        "                       feature_cols=['dist_to_metro', 'total_sqft', 'bhk', 'bath'], # Corrected\n",
        "                       dependent='log_price',\n",
        "                       w=w)\n",
        "\n",
        "print(\"DONE\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}